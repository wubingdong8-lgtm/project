import type { NextRequest } from "next/server";
import { type MessageList } from "@/types";
import { createParser, ParsedEvent, ReconnectInterval } from "eventsource-parser";
import { MAX_TOKEN, TEAMPERATURE } from "@/utils/constant";
// ðŸ‘‡ 1. æ–°å¢žé€™ä¸€è¡Œ import
import { retrieveContext } from '../../utils/rag';

type StreamPayload = {
  model: string;
  messages: MessageList;
  temperature?: number;
  stream: boolean;
  max_tokens?: number;
}

export default async function handler(req: NextRequest,) {
  const { prompt, history = [], options = {} } = await req.json();
  const { max_tokens, temperature } = options

  // ==========================================
  // ðŸ”¥ã€æ’å…¥ä½ç½®ã€‘RAG é‚è¼¯å¾žé€™è£¡é–‹å§‹
  // ==========================================

  // 1. æ‹¿ä½¿ç”¨è€…çš„å•é¡Œ (prompt) åŽ»æ‰¾è³‡æ–™
  const context = retrieveContext(prompt);

  // 2. æº–å‚™ System Prompt (åŽŸæœ¬æ˜¯ç›´æŽ¥ç”¨ options.prompt)
  let systemPrompt = options.prompt || "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„ AI åŠ©æ‰‹";

  // 3. å¦‚æžœæœ‰æ‰¾åˆ°è³‡æ–™ï¼Œå°±åŠ åˆ°å¾Œé¢
  if (context) {
    // å¯ä»¥åœ¨ Vercel å¾Œå° logs çœ‹åˆ°æœ‰æ²’æœ‰æˆåŠŸ
    console.log("ðŸ” [RAG] æª¢ç´¢åˆ°çš„è³‡æ–™:", context); 
    
    systemPrompt += `\n\nã€ç³»çµ±è£œå…… - å­¸æ ¡å…§éƒ¨è³‡è¨Šã€‘ï¼š\n${context}\n\nè«‹å„ªå…ˆä¾æ“šä¸Šè¿°è£œå……è³‡è¨Šå›žç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚`;
  }

  // ==========================================
  // ðŸ”¥ã€æ’å…¥ä½ç½®ã€‘RAG é‚è¼¯çµæŸ
  // ==========================================

  const data = {
    model: "gpt-3.5-turbo",
    messages: [
      {
        role: "system",
        // ðŸ‘‡ 4. é€™è£¡åŽŸæœ¬æ˜¯ content: options.promptï¼Œæ”¹æˆæˆ‘å€‘åŠ å·¥éŽçš„è®Šæ•¸
        content: systemPrompt, 
      },
      ...history,
      {
        role: "user",
        content: prompt,
      },
    ],
    stream: true,
    temperature: +temperature || TEAMPERATURE,
    max_tokens: +max_tokens || MAX_TOKEN,
  };

  const stream = await requestStream(data);
  return new Response(stream);
}

// ... (ä¸‹æ–¹çš„ requestStream å’Œ createStream éƒ½ä¸ç”¨å‹•ï¼Œç¶­æŒåŽŸæ¨£å³å¯) ...

const requestStream = async(payload: StreamPayload) => {
  let counter = 0;
  const resp = await fetch(`${process.env.END_POINT}/v1/chat/completions`, {
    headers: {
      Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
      "Content-Type": "application/json",
    },
    method: "POST",
    body: JSON.stringify(payload),
  });
  if (resp.status !== 200) {
    return resp.body;
  }
  return createStream(resp, counter);
};

const createStream = (response: Response, counter: number) => {
  const decoder = new TextDecoder("utf-8"); // è½‰æ›æˆæ–‡å­—
  const encoder = new TextEncoder(); //è½‰æ›æˆäºŒé€²åˆ¶
  return new ReadableStream({
    async start(controller) {
      const onParse = (event: ParsedEvent | ReconnectInterval) => {
        if (event.type === "event") {
          const data = event.data;
          if(data === "[DONE]") {
            controller.close();
            return;
          }
          try {
            const json = JSON.parse(data)
            const text = json.choices[0]?.delta?.content || "";
            if(counter < 2 && (text.match(/\n/) || [].length)){
              return; //é‡åˆ°æ›è¡Œä¸è™•ç†
            }
            const q = encoder.encode(text);
            controller.enqueue(q);
            counter++;
          } catch (error){}
        }
      };

      const parser = createParser(onParse);

      for await (const chunk of response.body as any) {
        parser.feed(decoder.decode(chunk));
      }
    },
  });
};

export const config = {
  runtime: "edge",
};